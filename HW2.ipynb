{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Zc4sdy0s35D"
   },
   "source": [
    "# Создаём точку входа в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "idrMQH4dj605"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/26 16:17:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local[4]')\\\n",
    "        .appName('Lesson_2')\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .config('spark.executor.instances', 2)\\\n",
    "        .config('spark.executor.memory', '5g')\\\n",
    "        .config('spark.executor.cores', 2)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "ZfhfV3EWHzqB",
    "outputId": "ac1054eb-185e-4d53-ca98-7ad8b32c0119"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b75acab1142a:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lesson_2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7d9c9d610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrBE3PdXtOwT"
   },
   "source": [
    "# Самостоятельная работа\n",
    "\n",
    "Требуется выяснить:\n",
    "1. Какое соотношение сторон экрана телефона самое популярное,\n",
    "2. Плотность пикселей у экрана.\n",
    "\n",
    "Можно использовать только rdd.\n",
    "\n",
    "Всего 8 баллов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjAfrNtXSKBz"
   },
   "source": [
    "## Считывание данных\n",
    "Данные взяты отсюда: https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "Скачиваем и копируем в папку с нотебуком\n",
    "\n",
    "Внутри содержится следующая информация:\n",
    "\n",
    "* id: ID\n",
    "* battery_power: Total energy a battery can store in one time (mAh)\n",
    "* blue: Support bluetooth or not\n",
    "* clock_speed: Speed at which microprocessor executes instructions\n",
    "* dual_sim: Support dual sim or not\n",
    "* fc: Front Camera mega pixels\n",
    "* four_g: Support 4G or not\n",
    "* int_memory: Internal Memory (GB)\n",
    "* m_dep: Mobile Depth (cm)\n",
    "* mobile_wt: Weight of mobile phone\n",
    "* n_cores: Number of cores of processor\n",
    "* pc: Primary Camera mega pixels\n",
    "* px_height: Pixel Resolution Height\n",
    "* px_width: Pixel Resolution Width\n",
    "* ram: Random Access Memory (MB)\n",
    "* sc_h: Screen Height of mobile (cm)\n",
    "* sc_w: Screen Width of mobile (cm)\n",
    "* talk_time: Time that a single battery charge will last\n",
    "* three_g: Support 3G or not\n",
    "* touch_screen: Has touch screen or not\n",
    "* wifi: Support wifi or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "3VyKjYOSkLlp"
   },
   "outputs": [],
   "source": [
    "train = sc.textFile('train.csv')\n",
    "test = sc.textFile('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "E9Vb0d0hutEb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,battery_power,blue,clock_speed,dual_sim,fc,four_g,int_memory,m_dep,mobile_wt,n_cores,pc,px_height,px_width,ram,sc_h,sc_w,talk_time,three_g,touch_screen,wifi',\n",
       " '1,1043,1,1.8,1,14,0,5,0.1,193,3,16,226,1412,3476,12,7,2,0,1,0',\n",
       " '2,841,1,0.5,1,4,1,61,0.8,191,5,12,746,857,3895,6,0,7,1,0,0']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(3)\n",
    "test.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pNi9czovHNh"
   },
   "source": [
    "##  Преобразуем train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "8PzsLhWdvCY_"
   },
   "outputs": [],
   "source": [
    "train_first_row = train.first()\n",
    "\n",
    "train = train\\\n",
    "    .filter(lambda row: row != train_first_row)\\\n",
    "    .map(lambda row: [float(el) for el in row.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "QKCMFiVenNia"
   },
   "outputs": [],
   "source": [
    "# Преобразуйте test (1 балл)\n",
    "###############\n",
    "test_first_row = test.first()\n",
    "\n",
    "test = test\\\n",
    "    .filter(lambda row: row != test_first_row)\\\n",
    "    .map(lambda row: [float(el) for el in row.split(',')])\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjCdeAS4v1Vc"
   },
   "source": [
    "## Объединим train и test\n",
    "Найти нужную функцию можно [здесь](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html#rdd-apis)\n",
    "\n",
    "PS: нужно сделать средсвтвами rdd pd.concat([train, test,], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы имеют разную структуру, надо их чуть доработать перед объединением. Переложу id в конец а перед ним добавлю null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battery_power,blue,clock_speed,dual_sim,fc,four_g,int_memory,m_dep,mobile_wt,n_cores,pc,px_height,px_width,ram,sc_h,sc_w,talk_time,three_g,touch_screen,wifi,price_range'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,battery_power,blue,clock_speed,dual_sim,fc,four_g,int_memory,m_dep,mobile_wt,n_cores,pc,px_height,px_width,ram,sc_h,sc_w,talk_time,three_g,touch_screen,wifi'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(lambda x: [x[i+1] if i<len(x)-1 else x[0] if i>=len(x)-1 and i<len(x) else 'Null' for i in range(len(x)+1)])\n",
    "train = train.map(lambda x: [x[i] if i<len(x)-1 else 'Null' if i==len(x)-1 else x[len(x)-1] for i in range(len(x)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "5-Oeq54KxhXa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объедините train и test (2 балла)\n",
    "###############\n",
    "data = train.union(test)\n",
    "data.count()\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxGGHqRyx7XG"
   },
   "source": [
    "## Рассчитайте соотношение сторон телефона и экрана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "HckZvR72xs7W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 1180), (True, 1525)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############\n",
    "sc_h = train_first_row.split(',').index('sc_h')\n",
    "sc_w = train_first_row.split(',').index('sc_w')\n",
    "px_h = train_first_row.split(',').index('px_height')\n",
    "px_w = train_first_row.split(',').index('px_width')\n",
    "\n",
    "# Проведена подготовка данных для следующих двух задач\n",
    "###############\n",
    "# Выведите отсортированное распределение соотношений сторон экрана(1 балла)\n",
    "# в разрезе широкоформатные или нет (экран широкоформатный, если соотногшение >=16:9)\n",
    "data.filter(lambda x: x[sc_h] and x[sc_w] and x[px_h] and x[px_w] != 0)\\\n",
    "    .map(lambda x: (True if x[sc_h]/x[sc_w] >= 16/9 else False, 1)).reduceByKey(lambda x, y: x + y ).sortByKey().collect()\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "yZgL4q1gyrRm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.5904687951745147, 1),\n",
       " (3.589853511846724, 1),\n",
       " (3.5892113392903413, 1),\n",
       " (3.588206634400688, 1),\n",
       " (3.5873525385002627, 1),\n",
       " (3.586105251479891, 1),\n",
       " (3.5860973021333273, 1),\n",
       " (3.5858715969308963, 1),\n",
       " (3.584746759362444, 1),\n",
       " (3.5845846147185196, 1),\n",
       " (3.583112254242845, 1),\n",
       " (3.5829082575892466, 1),\n",
       " (3.5822692516077166, 1),\n",
       " (3.5822604595401337, 1),\n",
       " (3.580966403630147, 1),\n",
       " (3.5805370424242176, 1),\n",
       " (3.5804704580307614, 1),\n",
       " (3.5800301806356547, 1),\n",
       " (3.580002717504524, 1),\n",
       " (3.5790705923998662, 1)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выведите отсортированное распределение плотности пикселей (1 балла)\n",
    "# точек на дюйм\n",
    "###############\n",
    "data.filter(lambda x: x[sc_h] and x[sc_w] and x[px_h] and x[px_w] != 0)\\\n",
    "    .map(lambda x: (((x[px_h]**2+x[px_w]**2)/(x[sc_h]**2+x[px_w]**2)/0.155)**0.5, 1)).sortBy(lambda x: x[0], ascending=False).take(10)\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNAGeyOmzEN4"
   },
   "source": [
    "# JOIN\n",
    "Повторите вышеописанное задание с помощью одной из функций ниже (отдельно рассчитайте для train и test, затем объедините результат)\n",
    "\n",
    "Пример для двух RDDs: (rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)})\n",
    "\n",
    "Имя функции |\tPurpose |\tExample |\tResult\n",
    "------------- |\t------- |\t------- |\t------\n",
    "subtractByKey |Remove elements with a key present in the other RDD.| rdd.subtractByKey(other) | {(1, 2)}\n",
    "join | Perform an inner join between two RDDs. | rdd.join(other) | {(3, (4, 9)), (3, (6, 9))}\n",
    "rightOuterJoin | Perform a join between two RDDs where the key must be present in the first RDD. | rdd.rightOuterJoin(other) | {(3,(Some(4),9)), (3,(Some(6),9))}leftOuterJoin | Perform a join between two RDDs where the key must be present in the other RDD. | rdd.leftOuterJoin(other) | {(1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9)))}\n",
    "cogroup | Group data from both RDDs sharing the same key. | rdd.cogroup(other) | {(1,([2],[])), (3,([4, 6],[9]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "kRb1q9QizDiH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.5904687951745147,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b190>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b490>)),\n",
       " (3.589853511846724,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b6d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b100>)),\n",
       " (3.5892113392903413,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b0a0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d86e8160>)),\n",
       " (3.588206634400688,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d86e8fd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d86e84f0>)),\n",
       " (3.5873525385002627,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d86e8ca0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d86e8cd0>)),\n",
       " (3.586105251479891,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d86e87f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779bc40>)),\n",
       " (3.5860973021333273,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779bb50>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779bb80>)),\n",
       " (3.5858715969308963,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779bbb0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779bd30>)),\n",
       " (3.584746759362444,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b280>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b7f0>)),\n",
       " (3.5845846147185196,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b940>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b850>)),\n",
       " (3.583112254242845,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b8b0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b820>)),\n",
       " (3.5829082575892466,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b730>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b790>)),\n",
       " (3.5822692516077166,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b490>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b6a0>)),\n",
       " (3.5822604595401337,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b670>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b550>)),\n",
       " (3.580966403630147,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b4f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b4c0>)),\n",
       " (3.5805370424242176,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b340>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b190>)),\n",
       " (3.5804704580307614,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779b1f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779b0d0>)),\n",
       " (3.5800301806356547,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779bdf0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779be20>)),\n",
       " (3.580002717504524,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779be80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779bee0>)),\n",
       " (3.5790705923998662,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779bf40>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779bfa0>)),\n",
       " (3.5790091638909294,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784040>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77840a0>)),\n",
       " (3.57705811408793,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784100>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784160>)),\n",
       " (3.575427982009222,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77841c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784220>)),\n",
       " (3.575335333054543,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784280>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77842e0>)),\n",
       " (3.5751058032124896,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784340>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77843a0>)),\n",
       " (3.575098909788753,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784400>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784460>)),\n",
       " (3.574639439120337,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77844c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784520>)),\n",
       " (3.574236687653039,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784580>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77845e0>)),\n",
       " (3.5736915296675087,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784640>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77846a0>)),\n",
       " (3.5734409352661425,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784700>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784760>)),\n",
       " (3.572484824501121,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77847c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784820>)),\n",
       " (3.5723869484099313,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784880>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77848e0>)),\n",
       " (3.572368276948624,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784940>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77849a0>)),\n",
       " (3.571579855323971,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784a00>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784a60>)),\n",
       " (3.5712915574831925,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784ac0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784b20>)),\n",
       " (3.571153989143364,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784b80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784be0>)),\n",
       " (3.5701221610709974,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784c40>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784ca0>)),\n",
       " (3.5699546675924454,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784d00>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784d60>)),\n",
       " (3.56917203968943,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784dc0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784e20>)),\n",
       " (3.5686687191552866,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784e80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784ee0>)),\n",
       " (3.567536027590353,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7784f40>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7784fa0>)),\n",
       " (3.5668431293806893,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d040>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d0a0>)),\n",
       " (3.56607163485423,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d100>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d160>)),\n",
       " (3.5660209669421157,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d1c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d220>)),\n",
       " (3.5655700334966562,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d280>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d2e0>)),\n",
       " (3.5651477993963137,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d340>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d3a0>)),\n",
       " (3.5636228632190448,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d400>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d460>)),\n",
       " (3.563271733167607,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d4c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d520>)),\n",
       " (3.562248715364907,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d580>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d5e0>)),\n",
       " (3.5619903700564297,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d640>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d6a0>)),\n",
       " (3.559838192437641,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d700>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d760>)),\n",
       " (3.559504804336832,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d7c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d820>)),\n",
       " (3.5594666967994146,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d880>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d8e0>)),\n",
       " (3.558014601785373,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779d940>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779d9a0>)),\n",
       " (3.557474533814854,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779da00>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779da60>)),\n",
       " (3.5572535134491767,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779dac0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779db20>)),\n",
       " (3.556951449826517,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779db80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779dbe0>)),\n",
       " (3.5566200569851762,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779dc40>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779dca0>)),\n",
       " (3.556367248061103,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779dd00>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779dd60>)),\n",
       " (3.554444277501784,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779ddc0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779de20>)),\n",
       " (3.5544356267867445,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779de80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779dee0>)),\n",
       " (3.5542271727457133,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a779df40>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a779dfa0>)),\n",
       " (3.5526717468052675,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7040>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b70a0>)),\n",
       " (3.552414710348031,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7100>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7160>)),\n",
       " (3.551675459314987,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b71c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7220>)),\n",
       " (3.550452739233193,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7280>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b72e0>)),\n",
       " (3.5487075353744633,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7340>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b73a0>)),\n",
       " (3.5485861926176887,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7400>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7460>)),\n",
       " (3.548155352430004,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b74c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7520>)),\n",
       " (3.5474975211448476,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7580>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b75e0>)),\n",
       " (3.547416129394077,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7640>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b76a0>)),\n",
       " (3.5472912680148454,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7700>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7760>)),\n",
       " (3.54709285638043,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b77c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7820>)),\n",
       " (3.547055010484443,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d86a2b80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7ebe0>)),\n",
       " (3.546468200747523,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7ea60>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7eb20>)),\n",
       " (3.546455638864618,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7eac0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77a2f10>)),\n",
       " (3.5460920598404724,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77a2880>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77a2b20>)),\n",
       " (3.545920162210628,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8a8b640>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8a8bb50>)),\n",
       " (3.5451288555387013,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8a8bcd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77bc550>)),\n",
       " (3.5439168371019445,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77bc4f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d889df70>)),\n",
       " (3.5432826025557125,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d86968e0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8696670>)),\n",
       " (3.5431163322516124,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8696850>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8696bb0>)),\n",
       " (3.5428804876084126,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8696160>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b9250>)),\n",
       " (3.5419255832535845,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b9f10>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b9130>)),\n",
       " (3.5419207486502025,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b9790>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b9730>)),\n",
       " (3.54149018152005,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b9df0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b9160>)),\n",
       " (3.5412822027049766,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b9490>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b9610>)),\n",
       " (3.5410674853741275,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d89006d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8975b50>)),\n",
       " (3.5408200525378137,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7ac3880>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d887f310>)),\n",
       " (3.5403468139019867,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c1f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c2b0>)),\n",
       " (3.54033087887438,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7cb50>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7cfa0>)),\n",
       " (3.538415539682247,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c3a0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7cd00>)),\n",
       " (3.5380369484381538,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c5b0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c5e0>)),\n",
       " (3.537386567910186,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c580>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c490>)),\n",
       " (3.5368170934320755,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a7b7c220>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8bdf910>)),\n",
       " (3.536171299105821,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8bdf850>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7d8bdfdc0>)),\n",
       " (3.5355650536318683,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7d8bdfb80>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7880>)),\n",
       " (3.535079355039546,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b78b0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7910>)),\n",
       " (3.534677866554145,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7970>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b79d0>)),\n",
       " (3.534620410915115,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fb7a77b7a30>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fb7a77b7a90>))]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 балла\n",
    "###############\n",
    "test_j = test.filter(lambda x: x[sc_h] and x[sc_w] and x[px_h] and x[px_w] != 0)\\\n",
    "    .map(lambda x: (((x[px_h]**2+x[px_w]**2)/(x[sc_h]**2+x[px_w]**2)/0.155)**0.5, 1)).sortBy(lambda x: x, ascending=False)\n",
    "\n",
    "train_j = train.filter(lambda x: x[sc_h] and x[sc_w] and x[px_h] and x[px_w] != 0)\\\n",
    "    .map(lambda x: (((x[px_h]**2+x[px_w]**2)/(x[sc_h]**2+x[px_w]**2)/0.155)**0.5, 1)).sortBy(lambda x: x, ascending=False)\n",
    "data_j = test_j.cogroup(train_j)\n",
    "data_j.sortBy(lambda x: x[0], ascending=False).take(10)\n",
    "\n",
    "###############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uySWyv8CXIMF"
   },
   "source": [
    "# DataFrame\n",
    "Теперь мы знаем про Dataframe. Нужно сделать практически всё то же самое, но используя датафрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "48HI_jrnbKEk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------------+------------------+\n",
      "|sc_w|sc_h|        widescreen|               PPI|\n",
      "+----+----+------------------+------------------+\n",
      "|   1|   5|               5.0|  5.97491029022372|\n",
      "|   1|   5|               5.0| 6.265384758716375|\n",
      "|   4|   5|              1.25|6.5931900549755245|\n",
      "|   1|   5|               5.0|6.4029318204693055|\n",
      "|   1|   5|               5.0| 8.280725156152835|\n",
      "|   4|   5|              1.25| 5.558948412862019|\n",
      "|   2|   5|               2.5| 8.187083201731864|\n",
      "|   2|   5|               2.5| 8.517260639796255|\n",
      "|   4|   5|              1.25| 7.598219353878171|\n",
      "|   2|   5|               2.5| 7.634570184561094|\n",
      "|   2|   5|               2.5| 8.752256362874766|\n",
      "|   3|   5|1.6666666666666667| 7.181404325672206|\n",
      "|   1|   5|               5.0| 8.231887775025994|\n",
      "|   4|   5|              1.25| 6.048125030798149|\n",
      "|   3|   5|1.6666666666666667| 7.622254386152439|\n",
      "|   1|   5|               5.0|6.7365723041473355|\n",
      "|   1|   5|               5.0| 6.897129738267312|\n",
      "|   1|   5|               5.0|  8.30378778302075|\n",
      "|   4|   5|              1.25| 7.160190705056452|\n",
      "|   4|   5|              1.25| 5.585536961805965|\n",
      "+----+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Считываем и объединяем данные (1 балл)\n",
    "# Приведите все данные к правильному типу, либо считайе сразу верно (1 балл)\n",
    "# Создаём колонки с соотношением сторон и плотностью пикселей (1 балл)\n",
    "###############\n",
    "df_train = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "df_test = spark.read.csv('test.csv', header=True, inferSchema=True)\n",
    "df = df_test.unionByName(df_train, allowMissingColumns = True)\n",
    "df.dtypes\n",
    "\n",
    "df.filter((df.sc_h != 0.0) & (df.sc_w != 0.0) & (df.px_width != 0.0) & (df.px_height != 0))\\\n",
    "    .withColumn('widescreen', df.sc_h/df.sc_w)\\\n",
    "    .withColumn('PPI', ((df.px_height**2+df.px_width**2)/(df.sc_h**2+df.sc_w**2)/(0.155))**0.155)\\\n",
    "    .select('sc_w', 'sc_h', 'widescreen', 'PPI').sort('sc_h', ascending=True).show()\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtJ2G-Eob6LI"
   },
   "source": [
    "## Сохранение\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "sgRM7IV3b6LJ"
   },
   "outputs": [],
   "source": [
    "# Сохраните результат в csv sep=';', encoding='cp1251'\n",
    "# с колонками id, плотность пикселей и временем разговора в формате \"1day 1hour 1minute\"\n",
    "# 2 балла\n",
    "###############\n",
    "df.filter((df.sc_h != 0.0) & (df.sc_w != 0.0) & (df.px_width != 0.0) & (df.px_height != 0))\\\n",
    "    .withColumn('widescreen', df.sc_h/df.sc_w)\\\n",
    "    .withColumn('PPI', ((df.px_height**2+df.px_width**2)/(df.sc_h**2+df.sc_w**2)/(0.155))**0.155)\\\n",
    "    .select('id', 'PPI', 'talk_time').sort('sc_h', ascending=True)\\\n",
    "    .repartition(1).write.option(\"header\", \"true\").csv('out.csv', sep=';', encoding='cp1251')\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mc9uG_y2bxV2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Урок 2. HW rdd df\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
